{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab2a9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests as rq\n",
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "import re, string\n",
    "from deep_translator import GoogleTranslator\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('brown')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467bddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from save file\n",
    "database = json.load(open(\"paintings_metadata_raw.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------Data Preprocessing-------------------\n",
    "# For descriptions missing in english, look for Dutch descriptions and translate them.\n",
    "# For completely missing descriptions, replace None english description with en empty String\n",
    "\n",
    "translator = GoogleTranslator(source = 'nl', target='en')\n",
    "\n",
    "for id in database:\n",
    "    \n",
    "    desc = database[id][\"artObject\"][\"label\"][\"description\"]\n",
    "    if desc == None:\n",
    "        desc_nl = database[id][\"artObject\"][\"description\"]\n",
    "        if desc_nl:\n",
    "            desc_en = translator.translate(desc_nl)\n",
    "        else:\n",
    "            desc_en = \"\"\n",
    "            \n",
    "        database[id][\"artObject\"][\"label\"][\"description\"] = desc_en\n",
    "        \n",
    "# Save processed metadata into file\n",
    "json.dump(database, open(\"paintings_metadata.json\", 'w' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6c5eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data from save file\n",
    "database = json.load(open(\"paintings_metadata.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ac4311",
   "metadata": {},
   "outputs": [],
   "source": [
    "paintings_descriptions = {}\n",
    "paintings_painter = {}\n",
    "\n",
    "for id in database:\n",
    "    desc = database[id][\"artObject\"][\"label\"][\"description\"]\n",
    "    if desc:\n",
    "        #print(desc)\n",
    "        paintings_descriptions[id] = desc\n",
    "    else:\n",
    "        paintings_descriptions[id] = \"\" #Description unavailable neither in Dutch or English\n",
    "    \n",
    "    painter = database[id][\"artObject\"][\"principalMaker\"]\n",
    "    if painter:\n",
    "        #print(painter)\n",
    "        paintings_painter[id] = painter\n",
    "    else:\n",
    "        paintings_painter[id] = \"\" #Painter Unvailable\n",
    "        \n",
    "# Save descriptions into file\n",
    "json.dump(paintings_descriptions, open(\"Paintings_descriptions.json\", 'w'))\n",
    "# Save painters into file\n",
    "json.dump(paintings_painter, open( \"Paintings_painters.json\", 'w'))\n",
    "\n",
    "del paintings_descriptions,paintings_painter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1776c5f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------NLP-----------------\n",
    "# For each painting, run three different nlp models (spacy, nltk, textblob) \n",
    "# and save the \"dirty\" findings along with IconClasses values extracted from metadata.\n",
    "# Exceptions will occur on objects that have no text description \n",
    "\n",
    "nlp_spacy = spacy.load('en_core_web_sm')\n",
    "\n",
    "uncleaned_objects = {}\n",
    "\n",
    "for id in database:\n",
    "    #print(id)\n",
    "    \n",
    "    desc = database[id][\"artObject\"][\"label\"][\"description\"]\n",
    "    \n",
    "    # get objects on painting from iconClass\n",
    "    objects_icon = database[id][\"artObject\"][\"classification\"][\"iconClassDescription\"]\n",
    "    \n",
    "    #print(objects)\n",
    "    nouns_spacy = []\n",
    "    try:\n",
    "        doc = nlp_spacy(desc.lower())\n",
    "    \n",
    "        for token in doc:\n",
    "            if token.pos_=='NOUN' and (token.dep_ in ['nsubjpass','conj']):\n",
    "                nouns_spacy.append(token.text)\n",
    "    except:\n",
    "        print(\"Error, spacy nlp failed on item: \"+id)\n",
    "    \n",
    "    nouns_textblob = []\n",
    "    try:\n",
    "        blob = TextBlob(desc.lower())\n",
    "        \n",
    "        nouns_textblob = [word for word, tag in blob.tags if tag in ('NN')]\n",
    "    except:\n",
    "        print(\"Error, textBlob nlp failed on item: \"+id)\n",
    "    \n",
    "    nouns_nltk = []\n",
    "    try:\n",
    "        tokenized = nltk.word_tokenize(desc.lower())\n",
    "        tagged = nltk.pos_tag(tokenized)\n",
    "        \n",
    "        for token in tagged:\n",
    "            if token[1]=='NN':\n",
    "                nouns_nltk.append(token[0])\n",
    "    except:\n",
    "        print(\"Error, NLTK failed on item: \"+id)\n",
    "        \n",
    "    uncleaned_objects[id] = list(set(objects_icon + nouns_spacy + nouns_nltk + nouns_textblob))\n",
    "\n",
    "    \n",
    "del database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ca89e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendValueToDictEntry(val, dest_dict, id):\n",
    "    \n",
    "    # add object in dict\n",
    "    if (id in dest_dict) and (val not in dest_dict[id]):\n",
    "        dest_dict[id].append(val)\n",
    "    else:\n",
    "        dest_dict[id] = list([val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ca4c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning of \"objects\" found in paintings. Various cleaning techniques are applied.\n",
    "\n",
    "sw = set(nltk.corpus.stopwords.words('english'))\n",
    "puncs = list(set(string.punctuation)); puncs.append(\"‘\"); puncs.append('–'); puncs.append('’')\n",
    "\n",
    "objects_cleaned = {}\n",
    "\n",
    "# clean set of objects\n",
    "for id in uncleaned_objects:\n",
    "    #print(id)\n",
    "    for word in uncleaned_objects[id]:\n",
    "        # print(word)\n",
    "        \n",
    "        # remove if word is larger than 3 words\n",
    "        if len(word.split()) > 3:\n",
    "            continue\n",
    "    \n",
    "        # remove any punctuation\n",
    "        word = re.sub(r'[^\\w\\s]','',word)\n",
    "        tokenized = word.split()\n",
    "        \n",
    "        for token in tokenized:\n",
    "            # skip letters, numbers, puncs\n",
    "            if (len(token) < 3) or token.isdigit() or (token in sw) or (token in puncs):\n",
    "                continue\n",
    "            \n",
    "            # add object in dict\n",
    "            appendValueToDictEntry(token, objects_cleaned, id)\n",
    "\n",
    "#del uncleaned_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b548fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordnet usage analysis\n",
    "# filter out anything that is not considered lexicographically as Food, Plant, Animal or Artifact\n",
    "# also, convert any words in plural to singular\n",
    "\n",
    "objects_final = {}\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "for id in objects_cleaned:\n",
    "    #print(id)\n",
    "    for word in objects_cleaned[id]:\n",
    "        \n",
    "        word = word = lem.lemmatize(word)\n",
    "        syns = wn.synsets(word, pos = wn.NOUN)\n",
    "        \n",
    "        for syn in syns:\n",
    "            if any(x in syn.lexname() for x in ['food','plant','animal','artifact']):\n",
    "                appendValueToDictEntry(word, objects_final, id)\n",
    "                break\n",
    "                \n",
    "#del objects_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfe476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hypernyms. For exmaple, if both \"insect\" and \"butterfly\" are on list, remove \"insect\"\n",
    "\n",
    "for id in objects_final:\n",
    "    for word in objects_final:\n",
    "        syns = wn.synsets(word, pos = wn.NOUN)\n",
    "        for syn in syns:\n",
    "            for hypernym in syn.hypernyms():\n",
    "                if any(x in hypernym.name() for x in objects_final):               \n",
    "                    for hyper_word in objects_final:\n",
    "                        if hyper_word in hypernym.name() and hyper_word != word:\n",
    "                            objects_final[id].remove(hyper_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdcd4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save objects into file\n",
    "json.dump(objects_final, open(\"objects_nlp.json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f55bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for id in objects_final:\n",
    "#     print(\"ID: \", id)\n",
    "#     print(\"Uncleaned: \", uncleaned_objects[id])\n",
    "#     print(\"Cleaned: \", objects_cleaned[id])\n",
    "#     print(\"Final: \", objects_final[id])\n",
    "#     print(\"--------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
